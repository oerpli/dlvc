% !TeX root = ../dlvc.tex
\newcommand{\ttitle}{Image Classification using Softmax Classifier and CNN \TODO}
\hyphenation{}
\maketitle
\keywords{stochastic gradient descent, convolutional neural network}


\begin{abstract}
This is the report for the second assignment of the course ``Deep Learning for Visual Computing'' in the winter term 2016 at the TU Vienna. The task was to build a linear model to classify images of  the CIFAR10 dataset \TODO
\end{abstract}


\section{Deep Learning Libraries}
% Briefly state which deep learning library you used and if you used the server for training. One or two sentences are sufficient.
We used Keras with tensorflow as backend for training. We used the server since we could not get the system running on a Windows platform. For development, we used Keras with theano backend which caused a lot of trouble due to the different channel ordering. 


\section{Linear Models}
In the last assignment we ran into some performance issues as a kNN classifier has to compare each input feature vector to all (at least in the unoptimized version) the feature vectors in the training set. Without further optimization the runtime for each sample is therefore linear in the size of the training set. There are datastructures that improve this proceess (e.g. storing training set in a KDTree) but the performance still remains a limiting factor. A parametric model tries to solve this problem by finding parameters that describe the function that maps  input vectors to the class prediction (output vector).
% Describe what parametric models are and how they differ from the k nearest neighbors classifier we used in Assignment 1. Describe what a linear model is and how these models are used for classification. What do the parameters (weights and biases) of linear classifiers specify?

A linear model is one of the easiest parametric models --- it consists of a weight matrix $\textbf W$ and a bias vector $b$. The weight matrix maps the input vector to some other vector and the bias is then added to it (Eq.~\eqref{eq:lm}). Therefore $\textbf W$ basically consists of $T$ vectors that give the slope of the decision boundaries whereas $b$ specifies the offset. A linear model for a $N$ dimensional feature vector and $T$ classes therefore has $N \cdot T + T$ parameters (around 30k in our case).
\begin{align}
 w = f(x, (\textbf W,  b)) =\textbf  Wx +  b \label{eq:lm}
\end{align}
The resulting $T$ (number of classes) dimensional vector $w$ is then normalized with the softmax function (maps values to $(0,1)$ range s.t. they sum up to $1$) and the component with the highest value is used as a class prediction for the the input sample.

As the name already suggests the resulting model linearly seperates the feature space with $T$ decision boundaries (hyper planes) --- one for each class. The distance from an input vector to the decision boundary of class $t$ gives the score for the given class. If the data is linearly separable this can work quite well --- there are improvements to this method, e.g. Support Vector Machines that try to maximize the distance between the two classes. There are also extensions to SVM where a modified ``kernel'' is used to get nonlinear decision boundaries with only constant overhead. Nevertheless this methods aren't really useful for tasks such as image prediction as the decision boundaries are highly complex.

\section{Minibatch Gradient Descent}
%Explain the purpose and tasks involved in training parametric models.
The goal of training parametric models is to find parameters $\theta$ s.t. the function $f(\cdot, \theta)$ is as close as possible to the real underlying distribution. Therefore the training process tries to maximize the similarity of the resulting distribution with the distribution from the trainings set.
%Give an overview of gradient descent and loss functions in general as well as cross-entropy loss.
The cross entropy is a measure for the similarity of two probability vectors which penalizes uncertainty (uniform distributions) as well as misclassification. Summing up the cross entropy for all samples for a given parameter $\theta$ gives the loss value $L(\theta$) which should be minimized by finding the best parameters. Finding the global minimum of a complex function such as $\L(\theta)$ is in general an unsolvable task -- it's therefore necessary to rely on heuristics, such as the gradient descent method which works as follows:
\begin{enumerate}
\item Start at some random position $\theta$ in parameter space (i.e. initalize $\textbf W, b$ with random values)
\item Calculate gradient of the loss function, i.e. $\theta' = \nabla L(\theta)$ (the result is a vector where each component is equal to the derivation of $L(\theta)$ in the direction of the component) 
\item Set new parameter as $\theta  - \alpha \theta'$, i.e. move in the direction of the steepest descent with step size proportional to the learning rate $\alpha$.
\item Repeat until $\theta$ converges (this is the case if the gradient is zero in all directions, i.e. a local/global minimum, or at least a critical point) or other stopping criterion is reached
\end{enumerate}
%For instance, you could explain gradient descent using the hiking analogy I used in the lecture.
The idea behind this algorithm is that the Loss function is somewhat smooth (to be precise: differentiable, i.e. at every point there is a linear approximation, meaning that the limes' coming from each side converge to the same value) and therefore following the slope leads to a minimum. This approach has two problems
\begin{itemize}
\item It can get stuck in a local optimum
\item It does not scale well with huge datasets
\end{itemize}
%What is minibatch gradient descent and why is it preferred to batch gradient descent?
To alleviate this problems stochastic gradient descent\footnote{Or \textit{minibatch gradient descent} as not only one sample is used} is used. For this method the gradient is not calculated with all samples in the dataset but by partitioning the dataset into random subsets of equal size and calculating an estimate of the gradient with each subset and make a step in this direction. In our case we used 64\footnote{We also tried 32 and 128 but the accuracy did not differ significantly. The performance may have been affected though this was difficult to judge as the runtime  fluctuated heavily even when training with the same parameters.} samples per minibatch. This obviously makes calculating the gradient cheaper as only 64 instead of 40000 samples have to be considered each time. Furthermore it adds a random component to the procedure as a local optimum of the loss function of the whole dataset is usually not a local optimum of each subset --- therefore it helps with escaping local optima. To reduce the impact of the learning-rate-parameter we used \textit{momentum}. The idea of this is to accelerate the convergence by increasing the step size if the gradient points in the same direction in successive steps. 
\section{Preprocessing}
There are two goals of preprocessing --- helping with the training of the classifer and removing undesired influences from our data. In our case we only applied methods for the first part of this. We applied the following sequence of transformations to our images:
\begin{itemize}
\item Resize: If an image has the wrong size it will be scaled down to match the input size of our classifier (does only apply to images not in the dataset that we want to classify)
\item Casting the RGB values to float -- this has no purpose by itself but enables the following transformations
\item Subtraction of mean: We calculate the mean of each color channel and subtract this value from the features of this channel  
\item Division by standard deviation: Calculate the standard deviation of each color chnalle and divide the features of each channel by their respective value. 
\end{itemize}
The first one does not have any significance for the training of the model --- it's only used to enable classify images that have not the same size as the training data. The last two transformations are those that help with training the model by adjusting the data in a way that each feature has a mean of zero and standard deviation of one (over the whole dataset). This helps with the training because if the features are normalized a specific feature does not dominate the gradient. This enables using a single value for the learning rate. This can also be done on a per pixel instead of per channel basis.

The other preprocessing methods that remove undesired influences from the input data would have to work on a per image basis. For example the mean of each image could be set to the mean of the whole dataset to get the same level of brightness or the range of the values could be scaled to use the whole spectrum for each image, resulting in a higher contrast. 

After calculating the means and standard deviations from the training set it is important to store those values somewhere because the same transformations (with means and standard dev. from the training set) have to be applied to the other datasets and to images that we want to classify with our model to ensure that they are from the same distribution. 

\section{Optimization vs. Machine Learning}
% Summarize the differences between pure optimization and machine learning. Describe what regularization is and how weight decay and early stopping work (no need to use formulas).
In ML we want to use the model trained on some data to classify new data. The goal is not only to find a model that minimizes the loss (by recoginizing each image in the training set) but a model that also works on data it has not seen before. Therefore it is not enough to optimize the values until the model recognizes each image in the training set (this could be achieved easier with a hashmap using the image vectors as keys and their respective classes as value) but to find parameters that work well on new data which usually correlates with working well on the test data (but not necessarily the other way around, see hashmap example).

Regularization techniques intend to improve the generality of the model, i.e. prevent overfitting. We applied two methods to achieve this:
\begin{itemize}
\item Early stopping: If the validation accuracy does not increase for a few epochs (we used 10 or 20) the process stops and uses the best model (judged by validation accurracy) found so far	
\item Weight decay: A term is added to the loss function to penalize large weights. This ensures that the outcome is not dominated by a single feature. The idea is very similar to shrinkage methods in linear regression (weight decay is basically a direct port of ridge regression for neural networks). 
\end{itemize}
\section{Convolutional Neural Networks}
The state of the art architecture for image classification are \emph{Convolutional Neural Networks} and they are the main topic of this lecture. The are based on the rather old concept of \emph{Perceptrons} introduced in the 1950s\cite{Rosenblatt1958}. It was based on how neurons in the brain work. The recent success is partly due to the layering of these neurons and advances in parallel computing on GPUs. A multi-layer-perceptron (MLP) is very powerful because it is an universal approximator.  It is a directed graph, with neurons as vertices who have an activation function and weighted, directes edges between them. We focus on feed-forward networks which are acyclic and where each level in the hierarchy is called a layer. A MLP has at least one input layer, several hidden layers and an output layer. On the input layer, the features of the dataset are fed into the network. The hidden layers perform the computation, and with at least one hidden layer an MLP can have a non-linear decision boundary. On the output layers, the network is mapped against the labels, at least for our classification tasks. For training, the output layer get a value of either 1 or 0, representing the classes. An MLP is connected in a dense manner, meaning all neurons of on layer are connected to all neurons of the next layer. This is impractical for image classification, since locally close pixels are more important than pixels far away. 
In contrast, \emph{Convolutional Neural Networks} are sparsely connected, usually (in our case at least) looking only into 3x3 sized grids. But by layering several of these steps, small concepts like edges on lower levels are combined into higher level features e.g. "cat-like" that finally cover big parts of the image. To achieve that, several specialized layers are needed:
\begin{itemize}
\item Convolutional layers have the same spatial dimensions as their input and each neuron is connected to a grid (eg. 3x3) from the previous layer. They extract a feature, and through parameter sharing they extract the same feature for the whole previous layer. To extract several features, several feature maps are needed.  
\item Pooling layers are used to reduce the spatial dimension. They are placed between the convolutional layers and combine the values of several pixels. One common type is the 2x2 max pooling, where the maximum value is taken from a 2x2 grid, and thus reducing the number of features by $\nicefrac 1 4$ (halving in each dimension of the 2D image). The final spatial resolution is usually <10 for each dimension. 
\item A fully connected layer is the last layer, where all output vertices are connected to the previous layer, so that the influence of each feature to each class is taken into account. This is where also the softmax functionality is added, ensuring that the resulting vector is a probability distribution predicting the class. 
\end{itemize}

\section{Implementation and results}

\subsection{Softmax training}
As expected the softmax classifier without early stopping had a rising training accuracy (97.1\% after 200 epochs) though the validation accuracy did not improve much from the initial results after a few epochs and peaked at 30.7\%. Using the HOG feature vectur and early stopping resulted in an accuracy of 40.8\%. For both methods training and validation accuracy are plotted in Fig.~\ref{fig:accuracy}.
\begin{figure}[h!t]
\newcommand{\plotref}[1]{{[~\ref{plt:#1}~]}}
\centering
\input{./img/a2-softmax-c10.tex} % TODO - update data files for both lines and add plots for other images
\caption{Accuracy of Softmax classifier on raw images of TinyCifar10 images and on HOG vectors. Using HOG features instead of the raw images leads to significantly higher validation accuracy. Classifier on raw images primarily overfits (increasing training accuracy but sinking validation accuracy). } %\plotref{t1}.
\label{fig:accuracy}

\end{figure}

\begin{figure}[h!t]
\newcommand{\plotref}[1]{{[~\ref{plt:#1}~]}}
\centering
\input{./img/a2-softmax-c10.tex} % TODO - update data files for both lines and add plots for other images
\caption{Accuracy of softmax classifier and CNN on raw images of TinyCifar10 images and on HOG vectors (only softmax classifier). Using HOG features instead of the raw images leads to significantly higher validation accuracy for the softmax classifier. Softmax classifier on raw images primarily overfits (increasing training accuracy but sinking validation accuracy). Plots only until early stopping (in the case of softmax classifier on raw data  } %\plotref{t1}.
\label{figparam}

\end{figure}

\subsection{Softmax hyperparameter optimization}


\begin{figure}[h!t]
\newcommand{\plotref}[1]{{[~\ref{plt:#1}~]}}
\centering
\input{./img/a2-opt-hog6.tex} % TODO - update data files for both lines and add plots for other images
\caption{Accuracy of Softmax classifier on HOG vectors of TinyCifar10. } %\plotref{t1}.
\label{figparam-opt-hog6}
\end{figure}

We used a grid search for finding good parameters for weight decay and learning rate. Using very extreme values (see Fig.~\ref{figparam-opt-hog6} we were able to show that most combinations yield good results, but that a low learning rate and a high weight decay lead to a model with far too less training and thus bad results. Our first tests were from that range since the assignment depicts them as good values. But in general, it is fair to say that HOG features instead of the raw images lead to significantly higher validation accuracy with this simple softmax classifier.

\subsection{CNN}
We more or less followed the task assignmend step by step. We had some problems due to the variable ordering and with outdated libraries on the server (at least it did not recognize the preserve\_range argument with python3) but apart from that everything worked more or less as expected. The classifier had quite good results, reaching 71.3\% validation accuracy and 70.4\% test accuracy (see Fig.~\ref{fig:accuracy}). The model with which we achieved these values is also submitted together with this report.

\subsubsection{Classifying image with CNN}
In our testing with various cat images we often got dogs, deer and even frogs as result. To analyse why this is the case we looked at the missclassification rates for the 10 classes and saw, that the classifier has big problems with cats (48\% misclassification rate). If a sample wasn't correctly classified it almost never was classified as an automobile but very often as a cat or a deer, see Fig.~\ref{fig:missrate}.


\begin{figure}[h!t]
\newcommand{\plotref}[1]{{[~\ref{plt:#1}~]}}
\centering
\input{./img/missrate.tex} % TODO - update data files for both lines and add plots for other images
\caption{The  blue bars \plotref{mb} are the misclassification rates per class with the CNN classifier. The red bars \plotref{mr} are the chance that a missclassified image ends up in the given class. Usually a confusion matrix would be used for this kind of visualiation to find out which classes are confused by the classifier.} %\plotref{t1}.
\label{fig:missrate}

\end{figure}



