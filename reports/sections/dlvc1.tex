% !TeX root = ../dlvc.tex
\newcommand{\ttitle}{kNN Classification and Histograms of Oriented Gradients}
\hyphenation{}
\maketitle

\keywords{kNN classifier, grid search, CIFAR10, histograms of oriented gradients}


\begin{abstract}
This report is a summary of the work done for the first assignment of the course ``Deep Learning for Visual Computing'' in the winter term 2016 at the TU Vienna. The task was to implement a simple kNN classifier and tune the parameters with gridsearch on a subset of the CIFAR10 dataset.
\end{abstract}



\section{Image Classification} %Describe the image classification problem.
Image classification is one of the most basic tasks in visual computing. The goal is to have some decision procedure (an algorithm or a program usually) that is able to assign labels that should correspond to what a human would recognize on the image. The image is usually provided as a matrix of RGB values in some range, e.g. from 0 to 255 (8 bit per colour channel). The challenge is then, to associate concepts such as ``Skyscraper'' or ``Cat'' with this color information even though there exist a multitude of different skyscrapers and cats with varying colours and appearances.

\section{The CIFAR10 Dataset} % Describe the CIFAR10 dataset.
% https://www.cs.toronto.edu/~kriz/cifar.html

\section{Training, Validation and Test Sets} %Describe the purpose of these sets, why they are required, and how you obtained a validation set in case of CIFAR10.

\section{kNN Classifiers} %Describe how the kNN classifier works and how it can be used with images, which are not vectors. Explain what hyperparameters are in general and in case of kNN. How does parameter k generally influence the results?

\clearpage
\section{kNN Classification of CIFAR10} %Introduce the tiny version of CIFAR10. Describe what hyperparameter optimization is and why it is important. Explain your search strategy and visualize the results based on the output of knn_classify_tinycifar10.py.



\begin{figure}[h!]
\newcommand{\plotref}[1]{{[~\ref{plt:#1}~]}}
\centering
\input{./img/a1results.tex}
\caption{Accuracy of kNN classification with neighborhood sizes (k) and L1 and L2 norms on raw image and HOG vectors. Classification on HOG consistently outperforms class. on raw data and kNN with L1 norm is superior to kNN with L2 norm. Performance of best parameters ($k=21, \text{L1 norm}$) on test set with raw \plotref{t2} and HOG data \plotref{t1}. }

\end{figure}


\section{The Importance of Features} %Think about reasons why performing kNN classification directly on the raw images does not work well. Describe what a feature is and why operating on features instead of raw images is beneficial in case of kNN. Briefly explain what HOG features are. Compare the results when using these features (knn_classify_hog_tinycifar10.py) to those obtained using raw images and discuss them. Even with HOG features, the performance is still much lower than that of CNNs (90\% accuracy and more on the whole dataset). Think of reasons for why this is the case.
