% !TeX root = ../dlvc.tex
\newcommand{\ttitle}{Image Classification with CNN}
\hyphenation{}
\maketitle
%\keywords{stochastic gradient descent, convolutional neural network, softmax, multilayer perceptron}
\begin{abstract}
This is the report for the third assignment of the course ``Deep Learning for Visual Computing'' in the winter term 2016 at the TU Vienna. The task was to improve the missclassification rate of images of  the CIFAR10~\cite{CIFAR10} dataset by training data augmentation and a better architecture for the neural net.
\end{abstract}

\section{Deep Learning Libraries}
% Briefly state which deep learning library you used and if you used the server for training. One or two sentences are sufficient.
We used Keras with tensorflow as backend for training. We used the server since we could not get the system running on a Windows platform. 

\section{Image Transformations}
\subsection{Preprocessing}
\subsubsection{RGB to $YC_bC_r$}
The Idea of this approach was to change the color space from the standard RGB Model to the $YC_bC_r$ model, that has also three channel, but instead of three colors, one channel has the luminance information, while the other two have the chrominance information. This seems a more plausible input for an image classifier, since the luminance (similar to a B/W version) carries very different information than the chrominance, so seperating this early should benefit the process. 

\subsubsection{Other transformations}
Apart from this we only used the transformations already used for the last assignment. We thought about normalizing per color or similar, but we assumed that the improvement as well as performance impact would be in the same ballpark as the color space transformation and therefore didn't prioritize this highly and in the end we skipped it altogether to meet the deadline.


\subsection{Data augmentation}
As a neural network profits from a huge amount of training data we used data augmentation to generate additional samples from the CIFAR10 data by applying various transformations to it. We implemented a handful of transformations but in the end only used the subset which we though should work with the classification task at hand. All transformations had a parameter  for the probability that the transformation is applied. 
\subsubsection{Horizontal mirroring} Mirror the images along a vertical axis. Should not be used for tasks where this could be a problem (e.g. steering a a car is not a task that is invariant under this transformation)
\subsubsection{Vertical mirroring} We implemented this to test whether more training data is always better---our correct hypothesis was that this is not the case. The reason why this increases the missclassification ratio is that pictures usually have a correct up/down orientation and our validation set does not contain (much) vertically flipped images where it would be beneficial to train the classifier with samples that are augmented in this way. 
\subsubsection{Affine transformations} Instead of allowing general affine transformations we implemented a transformation that first applies a horizontal shear (to simulate taking the picture from another angle), then rotates the image by $90^\circ$ and applies another horizontal shear and rotates the image back again (to get a vertical shear) to simulate taking the picture from an elevated or lowered position. After this the image is rotated around the center. The maximal shear angles as well as the maximal rotation angle are parameters for this transformations which then applies this with a certain probability. The exact angles are sampled from a normal distribution with mean zero and standard deviation equal to $\nicefrac 1 2$ the maximum angle. All three affine transformations where combined to get one transformation to increase performance. If the image is rotated/sheared the empty area is filled up with black. With the values we chose for the max angle only a small part of the image was affected by this so we didn't think it's necessary to crop the black parts away.
\subsubsection{Gray scale} We calculated the mean of every pixel along the channel axis and assign this mean to all three channels to get identical images in grayscale. The idea here was that a cat/dog/car/\ldots could have various colors and it would therefore be a good idea to train the classifier on some images without the color information. We thought about extending this to not only have the original images and their b/w version but also something in between but it would have been computationally way more expensive and we assumed that it's not worth the computational overhead as the server was running at peak capacity most of the time and all groups would suffer from too many test runs.
\subsubsection{Random crop} This transformation takes three arguments, the desired minimum height and width as well as a probability. It returns an image with a size that is sampled from the uniform distribution between the minimum height/width and the original height/width. To get samples with consistent dimensions we upscaled the image back to $32\times 32$ with the resize transformation after applying the crop. 


\begin{align}
\left[\begin{array}{c}Y' \\ C_b \\ C_r\end{array}\right]
\approx
\left[\begin{array}{c}0  \\ 128 \\ 128\end{array}\right]
+
\left[\begin{array}{ccc} 0.299 & 0.587 & 0.114\\ -0.168736 & -0.331264 & 0.5  \\ 0.5 & -0.418688 & -0.081312\end{array}\right]
\cdot
\left[\begin{array}{c}R'_d \\ G'_d \\ B'_d\end{array}\right]\label{eq:ycc}
\end{align}

\begin{figure}\centering
\includegraphics[width=1\textwidth]{./img/barn_rgb_ycc.jpg}
\captionof{figure}{Left RGB image, right the luminance and the 2 chrominance 
channels\cite{YCCWiki}}

\end{figure}

It is done using the transformation in \eqref{eq:ycc} and since our implementation was very inefficent, we used it as a preprocessing step rather than an online transformation. 
Unfortunately, the results were discouraging, the performance improved only slightly (below 1\%), which is below the fluctuations (due to random initalization and early stopping) we observed in various runs of identical models during the last assignment. Our explanation is that this is just a rather simple linear transformation of the input values, the network may already have done something similar on the first level. 




\section{Training the model}
After increasing the number of convolutional filters from the previous exercise we quickly had an improvement by $\approx 10\%$ in our validation accuracy. We employed different techniques to improve the validation accuracy from there but all efforts fell more or less flat. 

\subsection{Dropout}
Dropout is a technique that counteracts overfitting in a model. It sets values of random neurons in the affected layer to zero with the specified probability. This forces the model to consider different features in the input to assign a certain class to the output (e.g. if the ``fur'' feature is dropped the model could predict that a cat is not a human based on features like ``has four legs'' combined with ``has a tail''). 

In our first few tries we added a dropout after every pooling layer and measured the improvements for different values of the hyperparameter for the dropout probability. There the optimum was with $p=0.14$.
We also tried applying dropout to the input layer, as this was recommended in the keras documentation. This will remove some of the input neurons. In our case, it did worsen the results and that seems plausible, since the input neurons are highly correlated and this is the same as adding random noise to the image, which of course will weaken the classification task. A better approach would be darkening whole parts of the image to simulate occlusion.

In the end we used a dropout layer in the backend after the fully connected layer though we didn't tune the parameter there, due to time constraints. 
\subsection{Batch Normalization \TODO}

\subsection{Initialization}
We changed the random initialization to \lstinline{`he_normal'} \cite{he} as suggested in the lecture. The effect of this was negligible, we think this is due to small depth of our network. It may had a bigger role on a deeper network as ours only has a few layers (see section \ref{sec:a})
\subsection{Architectures}\label{sec:a}
We tried a few approaches but settled for a simple model in the end as there were some strange bugs problems with the sophisticated models. We looked for inspiration from submissions to previous contests on the CIFAR10 datase as well as in the literature and found a few models that seemed to be promising  (e.g. \cite{ref})  but resulted in a timeout on the server. Therefore we settled for less layers and, fewer convolutional filters and simpler backends. 
Our final model which we used to test the different strategies had the following layers (activation unless specified otherwise was ReLU, initialization wherever it applies was \lstinline{`he_normal'}.
\begin{enumerate}
\item conv $3\times 3@32$
\item pool $2\times 2, \text{stride} 2, \max$
\item conv $3\times 3@64$
\item pool $2\times 2, \text{stride} 2, \max$
\item conv $3\times 3@128$
\item conv $3\times 3@256$
\item pool $2\times 2, \text{stride} 2, \max$
\item flatten
\item FC 512
\item dropout
\item FC 10, softmax
\end{enumerate}

Adding additional pooling filters or a second dense layer in the backend strangely resulted in a completely useless classifier (all samples ended up in the same class).

\section{Testing the model}
For the testing script, we applied a variant of the oversampling method described in the lecture. We test different versions of the image (flipped, cropped, rotated) against our model an added up the probabilities. For that we used the same transformation we already applied in the data augmentation part. To get the final class, we chose the one with the highest combined probability. We could not assess the quality improvements of this method, because by the time we implemented that the long queues on the server permitted experiments (we tried to do the inference locally with the models trained on the server but this did not work -- the same test script produced $\approx 35\%$ accuracy instead of $78\%$ on the server). Though we assumed that it won't worsen the results, so it is part of our testing method. We would also have liked to choose only those top results, that differ the most from each other, but again, without experimenting, this part was scraped.

\section{Discussion}
All things considered we would say that we failed this task rather spectacularly. We estimated that we would need two weeks, where the first few days would be spent on implementing various augmentation techniques and the remaining time would be used to find an improved architecture for the network which we could then finetune. We think that this would have worked if we had had a CUDA enabled GPU at our disposal or if the server utilization had been similar to the previous assignment. Alas, this was not the case and we had to wait eight hours to 15 hours\footnote{We could have tested many differen techniques in parallel (exploring a few branches of the tree at once) to circumvent the downtime but this would have messed up the chances for other groups so we tried to utilize the server in a fair way} after every step to apply the next improvement. Finetuning the parameters was also difficult as the data augmentation required a different minibatch generator that could not cache the results and therefore tuning more parameters in one job would lead to a timeout. Resolving bugs in our architecture was also tedious due to this.
\\
The testing part is more or less missing as we wanted to implement this in the last two days but did not have a functioning model at the time. Using a model trained on the server to do inference on our local machines turned out to be impossible. We already suspected this when testing the classifier with $70\%$ from the previous assignment almost always misclassified test images on our machines though we assumed that this could be due to bad luck as we only tried a few images. This time we ran the identical testing script on the server and locally and got completely different results.  

