@misc{CIFAR10,
  title = {{The CIFAR-10 dataset}},
  howpublished = "\url{URL: https://www.cs.toronto.edu/~kriz/cifar.html/}",
  note = "[Accessed 5-November-2016]"
}%

@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

@article{Graham14a,
  author    = {Benjamin Graham},
  title     = {Fractional Max-Pooling},
  journal   = {CoRR},
  volume    = {abs/1412.6071},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6071},
  timestamp = {Thu, 01 Jan 2015 19:51:08 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Graham14a},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Dalal:2005:HOG:1068507.1069007,
 author = {Dalal, Navneet and Triggs, Bill},
 title = {Histograms of Oriented Gradients for Human Detection},
 booktitle = {Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Volume 1 - Volume 01},
 series = {CVPR '05},
 year = {2005},
 isbn = {0-7695-2372-2},
 pages = {886--893},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/CVPR.2005.177},
 doi = {10.1109/CVPR.2005.177},
 acmid = {1069007},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@article{simard,
  title={Efficient pattern recognition using a new transformation distance},
  author={Simard, Patrice and Denker, John S}
}

@article{Rosenblatt1958,
  author = {Rosenblatt, Frank},
  description = {Thesis BIB},
  journal = {Psychological Review},
  keywords = {imported},
  number = 6,
  pages = {386-408},
  timestamp = {2009-10-27T06:49:30.000+0100},
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  volume = 65,
  year = 1958
}


@inproceedings{he,
	address = {Washington, DC, USA},
	series = {{ICCV} '15},
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://dx.doi.org/10.1109/ICCV.2015.123},
	doi = {10.1109/ICCV.2015.123},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
	urldate = {2017-01-31},
	booktitle = {Proceedings of the 2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE Computer Society},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	pages = {1026--1034}
}


@article{ref,
	title = {Spatially-sparse convolutional neural networks},
	url = {http://arxiv.org/abs/1409.6070},
	abstract = {Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks. Motivated by the problem of online handwriting recognition, we developed a CNN for processing spatially-sparse inputs; a character drawn with a one-pixel wide pen on a high resolution grid looks like a sparse matrix. Taking advantage of the sparsity allowed us more efficiently to train and test large, deep CNNs. On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test error of 3.82\%. Although pictures are not sparse, they can be thought of as sparse by adding padding. Applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets: 6.28\% on CIFAR-10 and 24.30\% for CIFAR-100.},
	urldate = {2017-01-31},
	journal = {arXiv:1409.6070 [cs]},
	author = {Graham, Benjamin},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.6070},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1409.6070 PDF:C\:\\Users\\oerpli\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kohg7t4u.default\\zotero\\storage\\7B8HR9XM\\Graham - 2014 - Spatially-sparse convolutional neural networks.pdf:application/pdf}
}