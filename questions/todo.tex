% !TeX root = ./DLVCQuestions.tex

\section{Optimization in ML}
\subsection{What is the purpose of optimization in the context of machine learning?}
\subsection{How does the gradient descent algorithm work?}
\subsection{What is the gradient of a function? }


\section{Gradient descent}
\subsection{Consider the following contour plot of a function with two parameters. How might gradient descent proceed in this case, assuming the circle in the bottom-left corner as the starting point?}
\subsection{Mark the individual steps and connect them using lines.}
\subsection{Give a brief explanation of momentum.}
\subsection{How would momentum affect the training progress in the example case?}
\subsection{Mark the individual steps gradient descent with momentum might take, assuming the bottom-left square as the starting point}

\section{Weight and bias parameters}
\subsection{How do weight and bias parameters affect the input x?}
\subsection{What must be considered when initializing these parameters?}
\subsection{What happens if the weights are set too large or too small?}
\subsection{We discussed a heuristic for controlling the magnitude of weights. What is the intention behind this heuristic (no math required)?}

\section{Preprocessing}
\subsection{Explain the purpose of preprocessing.}
\subsection{How do per-sample normalization and per-trainingset normalization differ in terms of operation and purpose?}
\subsection{In the latter case, which (if any) preprocessing is applied during validation and testing?}

\section{Optimization vs ML}
\subsection{What are the goals of optimization and machine learning?}
In optimization the goal is to find an optimal value for a function, 
\subsection{Why do they differ?}
\subsection{Create two sketches with each showing the training progress over time in terms of both training and test error; one that is good from an optimization perspective but bad from a machine learning perspective, and one that is worse from an optimization perspective but better from a machine learning perspective. Explain both sketches.}

\section{Regularization}
\subsection{What is the purpose of regularization?}
\subsection{What is weight decay and what is its purpose?}
\subsection{What is early stopping and how does it work?}
\subsection{In deep learning, is it better to increase regularization or to decrease the model capacity by other means, and why?}

\section{Feedforward NN}
\subsection{What is the definition of a feedforward neural network?}
\subsection{Which types of units do such networks have?}
\subsection{Draw a graph of such a network.}

\section{MLP}
\subsection{What is the definition of a multilayer perceptron?}
\subsection{What operation do (non-input) units perform?}
\subsection{What is an activation function and which functions are common?}
\subsection{Draw a sketch that shows the layers of such networks and how the individual units are connected.}

\section{MLP for VC}
\subsection{Why are multilayer perceptrons not suitable for deep learning for image analysis?}

\section{Representation learning}
\subsection{What is the motivation and purpose for representation learning?}
\subsection{How is deep learning related to representation learning, and what is its definition?}

\section{Locally connected layers for image analysis}
\subsection{What is the motivation for using locally connected layers for image analysis?}
\subsection{What does sparse connectivity mean?}
\subsection{Assume input data of dimension $W \times H \times D$. How many many weight and bias parameters does a locally connected (but not convolutional) layer have, assuming $3 \times 3$ connectivity and $W \times H \times 2$ output data dimension, and why?}



\section{CNN}
\subsection{Give a general overview of convolutional neural networks and their purpose}
\subsection{Draw a sketch that illustrates their overall structure (typical layer types and their arrangement).}
\subsection{What are the two overall stages of such networks?}

\section{CNN depth}
\subsection{How is the depth of a CNN defined?}
\subsection{What effect does increasing the depth have?}
\subsection{How does one choose a suitable network depth to solve a given image classification problem?}

\section{CNN backend}
\subsection{What is the backend of a CNN?}
\subsection{Discuss the backends of VGGNet and GoogLeNet/ResNet and their pros and cons.}

\section{ResNets}
\subsection{What are residual networks (ResNets) and which problem do they overcome?}
\subsection{Explain what a residual block computes and create a sketch of such a block.}

\section{Dropout}
\subsection{What is the purpose of dropout, how does it work, and why is it effective?}
\subsection{Why do ``dropped'' neurons have no effect on the output of the next layer?}
\subsection{To which CNN layers is dropout commonly applied?}

\section{Batch normalization}
\subsection{What is the purpose and aim of batch normalization?}
\subsection{Why does it have a regularizing effect?}
\subsection{To which CNN layers is batch normalization applied and where?}

\section{Oversampling}
\subsection{What is the purpose of oversampling and how does it work?}
\subsection{What is ten-crop oversampling?}
\subsection{Assume the task of dog breed classification.}
\subsection{Think of and explain transformations that are applicable in this case. What about ten-crop oversampling?}

\section{Model ensembles}
\subsection{What is the purpose and intuition behind using model ensembles?}
\subsection{How might the individual models differ from each other?}
\subsection{How is the output of these models combined?}

\section{CNNs in medical imaging}
\subsection{What approaches are key components to use deep CNNs in medical imaging applications, especially when data for training is not or sparsely available?}
\subsection{Describe two approaches and the benefit of using them in medical imaging.}

\addtocounter{section}{-1}\section{}\label{sec:todo}